. Vision Transformer (ViT)

An Image is Worth 16ร16 Words: Transformers for Image Recognition at Scale

ููุณูุฏฺฏุงู: Alexey Dosovitskiy ู ููฺฉุงุฑุงู

ุณุงู / ูุญู ุงูุชุดุงุฑ: 2020 โ arXiv / ICLR

ุงุฏู ุงุตู:
ุงู ููุงูู ุจุฑุง ูุฎุณุชูโุจุงุฑ ูุดุงู ุฏุงุฏ ฺฉู ูโุชูุงู ุชุตูุฑ ุฑุง ุจู ูุฌููุนูโุง ุงุฒ ูพฺโูุง ุชูุณู ฺฉุฑุฏ ู ุขูโูุง ุฑุง ูุดุงุจู ุชูฺฉูโูุง ูุชู ุจู ฺฉ Transformer ุฏุงุฏุ ุจุฏูู ุงุณุชูุงุฏู ุงุฒ ุดุจฺฉูโูุง CNN.

ูฺฉุชู ฺฉูุฏ:
ุจุง ูพุดโุขููุฒุด ุฏุฑ ููุงุณ ุจุณุงุฑ ุจุฒุฑฺฏ ู ูุงูโุชูู ููุงุณุจุ ViT ุฑู ImageNet ุจู ุฏูุช ุจุณุงุฑ ุจุงูุง ุฏุณุช ูพุฏุง ูโฺฉูุฏ.

ุฏุชุงุณุชโูุง: JFT-300M (ูพุดโุขููุฒุด)ุ ImageNet (ุงุฑุฒุงุจ)

ูพุงุฏูโุณุงุฒ: JAX/Flax ู PyTorch

ุชุนุฏุงุฏ ุงุฑุฌุงุนุงุช: 75,000+

ฒ. Swin Transformer

Hierarchical Vision Transformer using Shifted Windows

ููุณูุฏฺฏุงู: Ze Liu ู ููฺฉุงุฑุงู

ุณุงู / ูุญู ุงูุชุดุงุฑ: 2021 โ ICCV

ุงุฏู ุงุตู:
ุงุณุชูุงุฏู ุงุฒ ูพูุฌุฑูโูุง ุชูุฌู ูุญู ุจุง ุฌุงุจูโุฌุง (Shifted Windows) ุจุฑุง ฺฉุงูุด ูุฒูู ูุญุงุณุจุงุช attention ู ุฏุฑ ุนู ุญุงู ุญูุธ ุงุฑุชุจุงุท ุจู ููุงุญ ุชุตูุฑ.

ูฺฺฏ ููู:
ุณุงุฎุชุงุฑ ุณูุณููโูุฑุงุชุจ ฺฉู ุขู ุฑุง ุจู ฺฉ backbone ุนููู ุจุฑุง ูุธุงู ูุฎุชูู ุจูุง ูุงุดู ุชุจุฏู ูโฺฉูุฏ.

ุฏุชุงุณุชโูุง: ImageNetุ COCOุ ADE20K

ูพุงุฏูโุณุงุฒ: PyTorch

ุชุนุฏุงุฏ ุงุฑุฌุงุนุงุช: 24,000+

ณ. Masked Autoencoders (MAE)

Masked Autoencoders Are Scalable Vision Learners

ููุณูุฏฺฏุงู: Kaiming He ู ููฺฉุงุฑุงู

ุณุงู / ูุญู ุงูุชุดุงุฑ: 2022 โ CVPR

ุงุฏู ุงุตู:
ูุงุณฺฉโฺฉุฑุฏู ุจุฎุด ุจุฒุฑฺฏ ุงุฒ ูพฺโูุง ุชุตูุฑ ู ุขููุฒุด ูุฏู ุจุฑุง ุจุงุฒุณุงุฒ ุขูโูุงุ ุจูโุนููุงู ฺฉ ุฑูุด ุณุงุฏู ุงูุง ุจุณุงุฑ ูุคุซุฑ ุฏุฑ ุงุฏฺฏุฑ ุฎูุฏูุธุงุฑุช.

ูุฒุช ุงุตู:
ุชููุฏ ููุงุดโูุง ูุฏุฑุชููุฏ ู ูุงุจู ุงูุชูุงู ุจู ูุธุงู ูุฎุชูู ูพุงูโุฏุณุช.

ุฏุชุงุณุช: ImageNet-1K

ูพุงุฏูโุณุงุฒ: PyTorch

ุชุนุฏุงุฏ ุงุฑุฌุงุนุงุช: 8,800+

ด. Segment Anything (SAM)

ููุณูุฏฺฏุงู: Alexander Kirillov ู ููฺฉุงุฑุงู

ุณุงู / ูุญู ุงูุชุดุงุฑ: 2023 โ ICCV

ุงุฏู ุงุตู:
ูุนุฑู ฺฉ ูุฏู ูพุงู ุจุฑุง ุชูุณูโุจูุฏ ุชุตูุฑ ฺฉู ูโุชูุงูุฏ ุจุง ุฏุฑุงูุช ูพุฑุงููพุช (ููุทูุ ฺฉุงุฏุฑ ุง ูุชู) ุชูุฑุจุงู ูุฑ ุดุฆ ุฑุง ุฏุฑ ุชุตูุฑ ุฌุฏุง ฺฉูุฏ.

ูฺฺฏ ูุชูุงุฒ:
ุขููุฒุด ุฑู ุฏุชุงุณุช ุนุธู SA-1B ุจุง ุจุด ุงุฒ ฺฉ ููุงุฑุฏ ูุงุณฺฉ.

ุฏุชุงุณุช: SA-1B

ูพุงุฏูโุณุงุฒ: PyTorch

ุชุนุฏุงุฏ ุงุฑุฌุงุนุงุช: 9,200+

ต. DINOv2

Learning Robust Visual Features without Supervision

ููุณูุฏฺฏุงู: Maxime Oquab ู ููฺฉุงุฑุงู

ุณุงู / ูุญู ุงูุชุดุงุฑ: 2023 โ arXiv

ุงุฏู ุงุตู:
ุงุฏฺฏุฑ ุฎูุฏูุธุงุฑุช ุฑู ูุฌููุนูโุฏุงุฏูโุง ุจุณุงุฑ ุจุฒุฑฺฏ ู ูพุงฺฉโุณุงุฒโุดุฏู ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุจุตุฑ ุนููู ู ููุงูู.

ูฺฉุชู ููู:
ูฺฺฏโูุง ุชููุฏุดุฏู ุญุช ุจุฏูู ูุงูโุชูู ูุฒ ุฑู ุจุณุงุฑ ุงุฒ ูุธุงู ุนููฺฉุฑุฏ ุจุงูุง ุฏุงุฑูุฏ.

ุฏุชุงุณุช: ุญุฏูุฏ 142 ูููู ุชุตูุฑ ุจุฏูู ุจุฑฺุณุจ

ูพุงุฏูโุณุงุฒ: PyTorch

ุชุนุฏุงุฏ ุงุฑุฌุงุนุงุช: 4,700+

๐ ุฌุฏูู ุฎูุงุตู ููุงูุงุช
#	ููุน ููุงูู	ุณุงู	ูุญู ุงูุชุดุงุฑ	ูุญูุฑ ุงุตู	ุฏุชุงุณุช	ฺฉุฏ	Citation
1	ูพฺููุด	2020	ICLR	ูุนุฑู ViT	JFT-300M, ImageNet	vision_transformer	75314
2	ูพฺููุด	2021	ICCV	Swin Transformer	ImageNet, COCO	Swin-Transformer	24313
3	ูพฺููุด	2022	CVPR	MAE	ImageNet-1K	mae	8846
4	ูพฺููุด	2023	ICCV	SAM	SA-1B	segment-anything	9223
5	ูพฺููุด	2023	arXiv	DINOv2	~142M images	dinov2	4734
๐ง ฺุฑุง ุงู ููุงูุงุช ุจู ูู ูุฑุชุจุทโุงูุฏุ

ููฺฏ ุฏุฑ ุญูุฒู ุงุฏฺฏุฑ ุนูู ู ููุด ูุตููุน ูุฑุงุฑ ุฏุงุฑูุฏ

ูุญูุฑ ูุดุชุฑฺฉ ุขูโูุง Transformerูุง ู ูุฏูโูุง ูพุงู ุฏุฑ ุจูุง ูุงุดู ุงุณุช

ฺฉ ูุณุฑ ุชฺฉุงูู ุฑุง ูุดุงู ูโุฏููุฏ:
ViT โ Swin โ MAE โ SAM โ DINOv2

๐ ูพูุฌ ฺฉูุฏูุงฺู ููู

Transformer Architecture
ุณุงุฎุชุงุฑ ูุจุชู ุจุฑ attention ฺฉู ูุณุชู ุงุตู ููู ุงู ูุฏูโูุงุณุช.

Self-Supervised Learning
ุงุฏฺฏุฑ ุจุฏูู ูุงุฒ ุจู ุจุฑฺุณุจุ ูพุงู MAE ู DINOv2 ู ุจุฎุด ููู ุงุฒ SAM.

Representation Learning
ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุนููู ู ุงูุชูุงูโูพุฐุฑ ุงุฒ ุฏุงุฏูโูุง ุฎุงู.

Foundation Models
ูุฏูโูุง ุจุฒุฑฺฏ ู ฺูุฏููุธูุฑู ฺฉู ุฑู ูุธุงู ูุฎุชูู ูุงุจู ุงุณุชูุงุฏูโุงูุฏ.

Vision Transformer (ViT)
ููุทู ุดุฑูุน ุชุญูู ุชุฑูุณููุฑูุฑูุง ุฏุฑ ุจูุง ูุงุดู.

๐ ุฌูุนโุจูุฏ

ุงู ูพูุฌ ููุงููุ ุณุฑ ุชุญูู ุจูุง ูุงุดู ูุฏุฑู ุฑุง ุงุฒ ูุฏูโูุง ุชุฎุตุต ุนูู ุจู ุณูุช ูุฏูโูุง ูพุงูุ ุนููู ู ฺูุฏููุธูุฑู ูุดุงู ูโุฏููุฏุ ูุณุฑ ฺฉู ุงุฒ ViT ุขุบุงุฒ ุดุฏู ู ุจู ูุฏูโูุง ูุงููุฏ SAM ู DINOv2 ุฑุณุฏู ุงุณุช
